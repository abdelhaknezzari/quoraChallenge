top10_2000  <- babynames %>% filter(year == 2000) %>%
group_by(sex) %>% top_n(10, prop) %>% arrange(sex, desc(prop))
View(top10_2000)
View(top10_2000)
ggplot(top10_2000, aes(x = forcats::fct_reorder(name, desc(prop)), y = prop)) +
geom_col() + facet_wrap(~ sex, scales = "free_x")
top10_2000_evolution  <- left_join(top10_2000 %>% select(name, sex), babynames)
left_join(top10_2000, babynames, by = c("name", "sex"))
ggplot(top10_2000_evolution, aes(x = year, y = prop, group = interaction(name, sex), color = name)) +
geom_line() + facet_wrap(~ sex)
babynames_diversity <-  babynames %>% group_by(year, sex) %>% top_n(10, prop) %>%
summarize(tot_prop = sum(prop))
ggplot(babynames_diversity, aes(x = year, y = tot_prop, color = sex)) + geom_line()
library(data.table)
babynames_dt <- data.table(babynames)
View(babynames_dt)
View(babynames_dt)
setkey(babynames_dt, prop)
top10_2000_dt <- babynames_dt[year == 2000, tail(.SD, 10), by = sex]
View(babynames_dt)
View(babynames_dt)
top10_2000_evolution_dt  <- babynames_dt[top10_2000_dt[,c("name","sex")], ,on = c("name", "sex")]
babynames_diversity_dt <- babynames_dt[, tail(.SD,10), by = c("year","sex")][, .(tot_prop = sum(prop)),by = c("year", "sex")]
library(RSQLite)
con <- dbConnect(SQLite(), "mydb.sqlite")
dbWriteTable(con, name = "babynames", babynames, overwrite = TRUE)
top10_2000_sql <- dbGetQuery(con, "SELECT * FROM (SELECT * FROM babynames WHERE (`year` == 2000) AND (sex == 'M') ORDER BY prop desc LIMIT 10)
UNION SELECT * FROM (SELECT * FROM babynames WHERE (`year` == 2000) AND (sex == 'F') ORDER BY prop desc LIMIT 10)")
dbWriteTable(con, name = "top10_2000", top10_2000, overwrite = TRUE)
top10_2000_evolution_sql <- dbGetQuery(con, "SELECT * FROM babynames JOIN (SELECT name, sex FROM top10_2000) USING (name, sex)")
babynames_mysql <- tbl(con, "babynames")
top10_2000_mysql  <- babynames_mysql %>% filter(year == 2000, sex == "M") %>% arrange(desc(prop)) %>% head(10)
n <- 5
res <- 1
for(i in 1:n){
res <- res * i
}
res
n <- 5
res <- 1
while(n > 0){
res <- res * n
n <- n - 1
}
prod(1:5)
factFor <- function(n){
res <- 1
for(i in 1:n){
res <- res * i
}
res
}
factWhile <- function(n){
res <- 1
while(n > 0){
res <- res * n
n <- n - 1
}
res
}
factRepeat <- function(n){
res <- 1
repeat{
res <- res * n
n <- n - 1
if(n == 0){
break
}
}
res
}
factProd <- function(n){
prod(1:5)
}
identical(factFor(n = n), factWhile(n = n))
identical(factFor(n = n), factRepeat(n = n))
trace(utils:::unpackPkgZip, edit=TRUE)
library(checkpoint)
install.packages("FactoMineR")
library(FactoMineR)
data(decathlon)
library(FactoMineR)
data(decathlon)
View(decathlon)
View(decathlon)
library(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 10, activation = 'softmax')
summary(model)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model %>% fit(
x_train, y_train,
epochs = 30, batch_size = 128,
validation_split = 0.2
)
View(mnist)
model %>% evaluate(x_test, y_test)
model %>% predict_classes(x_test)
library(readr)
library(keras)
library(purrr)
FLAGS <- flags(
flag_integer("vocab_size", 50000),
flag_integer("max_len_padding", 20),
flag_integer("embedding_size", 256),
flag_numeric("regularization", 0.0001),
flag_integer("seq_embedding_size", 512)
)
quora_data <- get_file(
"quora_duplicate_questions.tsv",
"http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv"
)
df <- read_tsv(quora_data)
tokenizer <- text_tokenizer(num_words = FLAGS$vocab_size)
fit_text_tokenizer(tokenizer, x = c(df$question1, df$question2))
question1 <- texts_to_sequences(tokenizer, df$question1)
question2 <- texts_to_sequences(tokenizer, df$question2)
question1 <- pad_sequences(question1, maxlen = FLAGS$max_len_padding, value = FLAGS$vocab_size + 1)
input1 <- layer_input(shape = c(FLAGS$max_len_padding))
input2 <- layer_input(shape = c(FLAGS$max_len_padding))
embedding <- layer_embedding(
input_dim = FLAGS$vocab_size + 2,
output_dim = FLAGS$embedding_size,
input_length = FLAGS$max_len_padding,
embeddings_regularizer = regularizer_l2(l = FLAGS$regularization)
)
seq_emb <- layer_lstm(
units = FLAGS$seq_embedding_size,
recurrent_regularizer = regularizer_l2(l = FLAGS$regularization)
)
vector1 <- embedding(input1) %>%
seq_emb()
vector2 <- embedding(input2) %>%
seq_emb()
out <- layer_dot(list(vector1, vector2), axes = 1) %>%
layer_dense(1, activation = "sigmoid")
model <- keras_model(list(input1, input2), out)
model %>% compile(
optimizer = "adam",
loss = "binary_crossentropy",
metrics = list(
acc = metric_binary_accuracy
)
)
set.seed(1817328)
val_sample <- sample.int(nrow(question1), size = 0.1*nrow(question1))
model %>%
fit(
list(question1[-val_sample,], question2[-val_sample,]),
df$is_duplicate[-val_sample],
batch_size = 128,
epochs = 30,
validation_data = list(
list(question1[val_sample,], question2[val_sample,]), df$is_duplicate[val_sample]
),
callbacks = list(
callback_early_stopping(patience = 5),
callback_reduce_lr_on_plateau(patience = 3)
)
)
save_model_hdf5(model, "model-question-pairs.hdf5", include_optimizer = TRUE)
save_text_tokenizer(tokenizer, "tokenizer-question-pairs.hdf5")
model <- load_model_hdf5("model-question-pairs.hdf5", compile = FALSE)
tokenizer <- load_text_tokenizer("tokenizer-question-pairs.hdf5")
predict_question_pairs <- function(model, tokenizer, q1, q2) {
q1 <- texts_to_sequences(tokenizer, list(q1))
q2 <- texts_to_sequences(tokenizer, list(q2))
q1 <- pad_sequences(q1, 20)
q2 <- pad_sequences(q2, 20)
as.numeric(predict(model, list(q1, q2)))
}
predict_question_pairs(
model, tokenizer,
q1 = "What is the main benefit of Quora?",
q2 = "What are the advantages of using Quora?"
)
library(readr)
library(keras)
library(purrr)
FLAGS <- flags(
flag_integer("vocab_size", 50000),
flag_integer("max_len_padding", 20),
flag_integer("embedding_size", 256),
flag_numeric("regularization", 0.0001),
flag_integer("seq_embedding_size", 512)
)
# Downloading Data --------------------------------------------------------
quora_data <- get_file(
"quora_duplicate_questions.tsv",
"http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv"
)
# Pre-processing ----------------------------------------------------------
df <- read_tsv(quora_data)
tokenizer <- text_tokenizer(num_words = FLAGS$vocab_size)
fit_text_tokenizer(tokenizer, x = c(df$question1, df$question2))
question1 <- texts_to_sequences(tokenizer, df$question1)
question2 <- texts_to_sequences(tokenizer, df$question2)
question1 <- pad_sequences(question1, maxlen = FLAGS$max_len_padding, value = FLAGS$vocab_size + 1)
question2 <- pad_sequences(question2, maxlen = FLAGS$max_len_padding, value = FLAGS$vocab_size + 1)
# Model Definition --------------------------------------------------------
input1 <- layer_input(shape = c(FLAGS$max_len_padding))
input2 <- layer_input(shape = c(FLAGS$max_len_padding))
embedding <- layer_embedding(
input_dim = FLAGS$vocab_size + 2,
output_dim = FLAGS$embedding_size,
input_length = FLAGS$max_len_padding,
embeddings_regularizer = regularizer_l2(l = FLAGS$regularization)
)
seq_emb <- layer_lstm(
units = FLAGS$seq_embedding_size,
recurrent_regularizer = regularizer_l2(l = FLAGS$regularization)
)
vector1 <- embedding(input1) %>%
seq_emb()
vector2 <- embedding(input2) %>%
seq_emb()
out <- layer_dot(list(vector1, vector2), axes = 1) %>%
layer_dense(1, activation = "sigmoid")
model <- keras_model(list(input1, input2), out)
model %>% compile(
optimizer = "adam",
loss = "binary_crossentropy",
metrics = list(
acc = metric_binary_accuracy
)
)
# Model Fitting -----------------------------------------------------------
set.seed(1817328)
val_sample <- sample.int(nrow(question1), size = 0.1*nrow(question1))
model %>%
fit(
list(question1[-val_sample,], question2[-val_sample,]),
df$is_duplicate[-val_sample],
batch_size = 128,
epochs = 30,
validation_data = list(
list(question1[val_sample,], question2[val_sample,]), df$is_duplicate[val_sample]
),
callbacks = list(
callback_early_stopping(patience = 5),
callback_reduce_lr_on_plateau(patience = 3)
)
)
install.packages("ImageNet")
? tune.grid
?? tune.grid
library(caret)
?? tune.grid
? expand.grid
expand.grid(height = seq(60, 80, 5), weight = seq(100, 300, 50),
sex = c("Male","Female"))
expand.grid(height = seq(0, 1, 0), weight = seq(1, 0, 1),
sex = c(1,0))
expand.grid(height = seq(0, 1, 0), weight = seq(1, 0, 1), sex = c(1,0) )
expand.grid(height = seq(1,2,3), weight = seq(1,2), sex = c(1) )
expand.grid(height = seq(1,2,3), weight = seq(1,2), sex = c(0,1) )
expand.grid(height = seq(1,2), weight = seq(1,2), sex = c(0,1) )
expand.grid(height = seq(1,5), weight = seq(1,2), sex = c(0,1) )
? seq
seq(-8,0, length.out = 10)
library(RNeo4j)
graph = startGraph("http://localhost:7474/db/data/")
var client = new GraphClient(new Uri("http://localhost:7474/db/data"), "neo4j", "Azronsen1988")
graph = startGraph("http://localhost:7474/db/data/")
?startGraph
graph = startGraph("http://localhost:7474/db/data/", "neo4j", "Azronsen1988")
graph
install.packages("RStoolbox")
trace(utils:::unpackPkgZip, edit=TRUE)
install.packages("RStoolbox")
library(RStoolbox)
library(sp)  # classes for spatial data
library(raster)  # grids, rasters
library(rasterVis)  # raster visualisation
library(maptools)
library(rgeos)
install.packages("rasterVis")
library(dismo)
install.packages("dismo")
mymap <- gmap("France")  # choose whatever country
plot(mymap)
library(dismo)
mymap <- gmap("France")  # choose whatever country
plot(mymap)
mymap <- gmap("Algeria")  # choose whatever country
plot(mymap)
mymap <- gmap("France", type = "satellite")
plot(mymap)
library(RgoogleMaps)
lat <- c(48,64) #define our map's ylim
lon <- c(-140,-110) #define our map's xlim
center = c(mean(lat), mean(lon))  #tell what point to center on
zoom <- 5
terrmap <- GetMap(center=center, zoom=zoom, maptype= "terrain", destfile = "terrain.png")
samps$size <- "small"
samps$col <- "red"
samps$char <- ""
mymarkers <- cbind.data.frame(samps$lat, samps$lon, samps$size, samps$col, samps$char)
names(mymarkers) <- c("lat", "lon", "size", "col", "char")
lat <- c(48,60)
lon <- c(-140,-110)
terrain_close <- GetMap.bbox(lonR= range(lon), latR= range(lat), center= c(49.7, -121.05), destfile= "terrclose.png", markers= mymarkers, zoom=13, maptype="terrain")
list.of.packages <- c("rgdal", "raster" "GSIF", "plotKML", "nnet", "plyr", "ROCR", "randomForest", "psych", "mda", "h2o", "dismo", "grDevices", "snowfall", "hexbin", "lattice", "ranger", "xgboost", "parallel", "doParallel", "caret")
list.of.packages <- c( "rgdal", "raster" "GSIF", "plotKML", "nnet", "plyr", "ROCR", "randomForest", "psych", "mda", "h2o", "dismo", "grDevices", "snowfall", "hexbin", "lattice", "ranger", "xgboost", "parallel", "doParallel", "caret" )
list.of.packages <- c( "rgdal", "raster" "GSIF", "plotKML", "nnet", "plyr", "ROCR", "randomForest", "psych", "mda", "h2o", "dismo", "grDevices", "snowfall", "hexbin", "lattice", "ranger", "xgboost", "parallel", "doParallel", "caret" )
list.of.packages <- c( "rgdal", "raster" "GSIF", "plotKML", "nnet", "plyr", "ROCR", "randomForest", "psych", "mda", "h2o", "dismo", "grDevices", "snowfall", "hexbin", "lattice", "ranger", "xgboost", "parallel", "doParallel", "caret" )
list.of.packages <- c( "rgdal", "raster","GSIF", "plotKML", "nnet", "plyr", "ROCR", "randomForest", "psych", "mda", "h2o", "dismo", "grDevices", "snowfall", "hexbin", "lattice", "ranger", "xgboost", "parallel", "doParallel" , "caret" )
installed <- installed.packages()
View(installed)
View(installed)
installed <- installed.packages(,"Package")
View(installed)
View(installed)
installed <- installed.packages()[,"Package"]
not.installed <- list.of.packages %in% installed
not.installed <- ! list.of.packages %in% installed
not.installed <- list.of.packages[! list.of.packages %in% installed]
View(not.installed)
install.packages(not.installed)
libaray(GSIF)
library(GSIF)
require(RCurl)
require(RCurl)
source_url <- function(url, ... )
{
cat(getURL(url, followlocation = TRUE, cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")), file = basename(url))
source(basename(url))
}
source_https("https://raw.githubusercontent.com/cran/GSIF/master/R/OCSKGM.R")
source_url("https://raw.githubusercontent.com/cran/GSIF/master/R/OCSKGM.R")
library(sp)
library(boot)
library(aqp)
library(sp)
library(boot)
library(aqp)
library(plyr)
library(rpart)
library(splines)
library(gstat)
library(quantregForest)
install.packages("quantregForest")
library(quantregForest)
library(plotKML)
demo(meuse, echo=FALSE)
om.rk <- predict(omm, meuse.grid)
omm <- fit.gstatModel(meuse, om~dist+ffreq, meuse.grid, method="quantregForest")
om.rk <- predict(omm, meuse.grid)
plotKML(om.rk)
plotKML(om.rk)
saga_cmd = "C:\Program Files (x86)\saga-6.3.0_x64\saga_cmd.exe"
saga_cmd = "C:/Program Files (x86)/saga-6.3.0_x64/saga_cmd.exe"
system(paste(saga_cmd))
system(paste(saga_cmd))
library(rgdal)
library(raster)
data("eberg_grid")
gridded(eberg_grid) <- ~x+y
proj4string(eberg_grid) <- CRS("+init=epsg:31467")
writeGDAL(eberg_grid["DEMSRT6"], "DEMSRT6.sdat", "SAGA")
system(paste(saga_cmd, 'ta_lighting 0 -ELEVATION "DEMSRT6.sgrd" -SHADE "hillshade.sgrd" -EXAGGERATION 2'))
saga_cmd = "C:/Program Files (x86)/saga-6.3.0_x64/saga_cmd.exe"
system(paste(saga_cmd))
library(rgdal)
library(raster)
data("eberg_grid")
gridded(eberg_grid) <- ~x+y
proj4string(eberg_grid) <- CRS("+init=epsg:31467")
writeGDAL(eberg_grid["DEMSRT6"], "DEMSRT6.sdat", "SAGA")
system(paste(saga_cmd, 'ta_lighting 0 -ELEVATION "DEMSRT6.sgrd" -SHADE "hillshade.sgrd" -EXAGGERATION 2'))
system(paste(saga_cmd))
saga_cmd = "C:\Program Files (x86)\saga-6.3.0_x64\saga_cmd.exe"
saga_cmd = "C:\\Program Files (x86)\\saga-6.3.0_x64\\saga_cmd.exe"
system(paste(saga_cmd))
saga_cmd
? system
system(paste('"c:/Program Files/Mozilla Firefox/firefox.exe"',
'-url cran.r-project.org'), wait = FALSE
)
system(paste('"C:/Program Files (x86)/saga-6.3.0_x64/saga_cmd.exe"'), wait = FALSE)
system(paste('"C:/Program Files/saga-6.3.0_x64/saga_cmd.exe"'), wait = FALSE)
system(paste('"C:/Program Files/saga-6.3.0_x64/saga_cmd.exe"'))
system(paste('"C:/Program Files/saga-6.3.0_x64/saga_cmd.exe"'))
library(rgdal)
library(raster)
data("eberg_grid")
gridded(eberg_grid) <- ~x+y
proj4string(eberg_grid) <- CRS("+init=epsg:31467")
writeGDAL(eberg_grid["DEMSRT6"], "DEMSRT6.sdat", "SAGA")
system(paste('"C:/Program Files/saga-6.3.0_x64/saga_cmd.exe"', 'ta_lighting 0 -ELEVATION "DEMSRT6.sgrd" -SHADE "hillshade.sgrd" -EXAGGERATION 2'))
system(paste('"C:/Program Files/saga-6.3.0_x64/saga_cmd.exe"', 'ta_lighting 0 -ELEVATION "DEMSRT6.sgrd" -SHADE "hillshade.sgrd" -EXAGGERATION 2'))
x = raster("hillshade.sdat")
x
plotKML(x)
? plotKML
View(x)
View(x)
SAGA_pal[[3]]
plotKML(x,colour_scale=SAGA_pal[[3]])
ls()
rm(ls())
ls() %>% rm()
rm(c)
rm(get(ls()))
rm(list=ls(all=TRUE))
ls(all=TRUE)
install.packages('lime')
trace(utils:::unpackPkgZip, edit=TRUE)
install.packages('lime')
q()
q()
trace(utils:::unpackPkgZip, edit=TRUE)
install.packages('koRpus')
library(koRpus)
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("trinker/textstem")
install.packages('sylly')
library(devtools)
install_github("unDocUMeantIt/sylly.en")
install_github("unDocUMeantIt/sylly")
library(devtools)
install_github("unDocUMeantIt/sylly.en")
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("trinker/textstem")
install_github("unDocUMeantIt/koRpus.lang.en")
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("trinker/textstem")
library(textstem)
shiny::runApp('RStudio/shiny-examples-master/082-word-cloud')
runApp('RStudio/shiny-examples-master/082-word-cloud')
load("df.RData")
if(df)
{}
exists("df")
"df" %>% exists()
nothing()
ifelse("df" %>% exists(), , df <- "http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv" %>%     get_file("quora_duplicate_questions.tsv", . ) %>% read_tsv()  )
ifelse("df" %>% exists(),NULL , df <- "http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv" %>%     get_file("quora_duplicate_questions.tsv", . ) %>% read_tsv()  )
df <- "http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv" %>%  get_file("quora_duplicate_questions.tsv", . ) %>% read_tsv()
quora_data <- get_file(
"quora_duplicate_questions.tsv",
"http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv" )
library(NLP)
library(openNLP)
library(openNLPdata)
library(readr)
library(keras)
library(purrr)
library(openNLPmodels.en)
library(dplyr)
library(data.table)
library(tidytext)
library(tidyr)
library(tokenizers)
library(TraMineR)
library(tm)
library(text2vec)
library(qdap)
library(qdapRegex)
library(stringr)
library(qdap)
library(stringi)
library(caret)
library(SnowballC)
library(lsa)
library(Rtsne)
library(ggrepel)
library(plot3D)
library(textstem)
library(textclean)
library(corrplot)
quora_data <- get_file(
"quora_duplicate_questions.tsv",
"http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv" )
ifelse("df" %>% exists(),NULL , df <- "http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv" %>%  get_file("quora_duplicate_questions.tsv", . ) %>% read_tsv()  )
ifelse("df" %>% exists(),NULL , df <- "http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv" %>%  get_file("quora_duplicate_questions.tsv", . ) %>% read_tsv()  )
ifelse("df" %>% exists(),NULL , load("df.RData")  )
ifelse("df" %>% exists(),1 , load("df.RData")  )
"df" %>% exists()
rm(df)
"df" %>% exists()
? exists
"df.RData" %>% file.exists()
ifelse("df.RData" %>% file.exists() ,load("df.RData") , "http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv" %>% get_file("quora_duplicate_questions.tsv", . ) %>% read_tsv())
df <- ifelse("df.RData" %>% file.exists() ,load("df.RData") , "http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv" %>% get_file("quora_duplicate_questions.tsv", . ) %>% read_tsv())
setwd('C:/Temp\X_Data_Science/Quora_Chalenge/Quora_chalenge_R/Quoura_Chalenge/shiny_for_question_similarities2')
setwd('C:/Temp/X_Data_Science/Quora_Chalenge/Quora_chalenge_R/Quoura_Chalenge/shiny_for_question_similarities2')
shiny::runApp()
runApp()
runApp()
