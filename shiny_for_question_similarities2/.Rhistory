{
tokenQ1 =   tokenize_words( OnRaw["question1"] ) %>% unlist() %>% sort() %>%  unique()
tokenQ2 =   tokenize_words( OnRaw["question2"] ) %>% unlist() %>% sort() %>%  unique()
d1 <- setdiff(tokenQ1, tokenQ2 )  %>% sort() %>%  unique()
d2 <- setdiff(tokenQ2, tokenQ1 )  %>% sort() %>%  unique()
switch( Option,
'1' = {
words_of_d1  <- d2      %>% word_vectors[., , drop = FALSE]
words_of_q2  <- tokenQ1 %>% word_vectors[., , drop = FALSE]
word_interest <- rbind(d2, tokenQ1)  %>% sort() %>%  unique()
} ,
'2' = {
words_of_d1  <- d1      %>% word_vectors[., , drop = FALSE]
words_of_q2  <- tokenQ2 %>% word_vectors[., , drop = FALSE]
word_interest <- rbind(d1, tokenQ2)  %>% sort() %>%  unique()
} ,
,
'default' = {
words_of_d1  <- d1      %>% word_vectors[., , drop = FALSE]
words_of_q2  <- tokenQ2 %>% word_vectors[., , drop = FALSE]
word_interest <- rbind(d1, tokenQ2)  %>% sort() %>%  unique()
}
)
word_interest_v <- word_interest %>%  word_vectors[., , drop = FALSE]
set.seed(50)
tsne_model_1 = word_interest_v %>%  as.matrix( ) %>%  Rtsne(. , check_duplicates=FALSE, pca=TRUE, perplexity=3, theta=0.5, dims=2)
## getting the two dimension matrix
d_tsne_1 = data.frame(tsne_model_1$Y, term =row.names(word_interest_v) )
row.names(d_tsne_1) =  row.names(word_interest_v)
ggplot(d_tsne_1, aes(x= X1, y = X2)) +
geom_point(color = "blue", size = 1) +
geom_label_repel(aes(label = term),
segment.color = "grey50") +
geom_segment(aes(x = 0, y = 0, xend = X1, yend = X2), data = d_tsne_1[tokenQ2,-3],arrow = arrow())+
geom_segment(aes(x = 0, y = 0, xend = X1, yend = X2, color="red"), data = d_tsne_1[d1,-3],arrow = arrow())+
geom_vline(xintercept = 0,color = "blue")+
geom_hline(yintercept = 0,color = "blue")+
theme_classic()
}
get_word_vectors() %>% plot_word_vector_questions_vectors(df[10,],.,'2')
get_word_vectors() %>% plot_word_vector_questions_vectors(df[10,],.,'3')
plot_word_vector_questions_vectors <- function(OnRaw,word_vectors,Option)
{
tokenQ1 =   tokenize_words( OnRaw["question1"] ) %>% unlist() %>% sort() %>%  unique()
tokenQ2 =   tokenize_words( OnRaw["question2"] ) %>% unlist() %>% sort() %>%  unique()
d1 <- setdiff(tokenQ1, tokenQ2 )  %>% sort() %>%  unique()
d2 <- setdiff(tokenQ2, tokenQ1 )  %>% sort() %>%  unique()
switch( Option,
'1' = {
words_of_d1  <- d2      %>% word_vectors[., , drop = FALSE]
words_of_q2  <- tokenQ1 %>% word_vectors[., , drop = FALSE]
word_interest <- rbind(d2, tokenQ1)  %>% sort() %>%  unique()
} ,
'2' = {
words_of_d1  <- d1      %>% word_vectors[., , drop = FALSE]
words_of_q2  <- tokenQ2 %>% word_vectors[., , drop = FALSE]
word_interest <- rbind(d1, tokenQ2)  %>% sort() %>%  unique()
} ,
'default' = {
words_of_d1  <- d1      %>% word_vectors[., , drop = FALSE]
words_of_q2  <- tokenQ2 %>% word_vectors[., , drop = FALSE]
word_interest <- rbind(d1, tokenQ2)  %>% sort() %>%  unique()
}
)
word_interest_v <- word_interest %>%  word_vectors[., , drop = FALSE]
set.seed(50)
tsne_model_1 = word_interest_v %>%  as.matrix( ) %>%  Rtsne(. , check_duplicates=FALSE, pca=TRUE, perplexity=3, theta=0.5, dims=2)
## getting the two dimension matrix
d_tsne_1 = data.frame(tsne_model_1$Y, term =row.names(word_interest_v) )
row.names(d_tsne_1) =  row.names(word_interest_v)
ggplot(d_tsne_1, aes(x= X1, y = X2)) +
geom_point(color = "blue", size = 1) +
geom_label_repel(aes(label = term),
segment.color = "grey50") +
geom_segment(aes(x = 0, y = 0, xend = X1, yend = X2), data = d_tsne_1[tokenQ2,-3],arrow = arrow())+
geom_segment(aes(x = 0, y = 0, xend = X1, yend = X2, color="red"), data = d_tsne_1[d1,-3],arrow = arrow())+
geom_vline(xintercept = 0,color = "blue")+
geom_hline(yintercept = 0,color = "blue")+
theme_classic()
}
get_word_vectors() %>% plot_word_vector_questions_vectors(df[10,],.,'3')
plot_word_vector_questions_vectors <- function(OnRaw,word_vectors,Option)
{
tokenQ1 =   tokenize_words( OnRaw["question1"] ) %>% unlist() %>% sort() %>%  unique()
tokenQ2 =   tokenize_words( OnRaw["question2"] ) %>% unlist() %>% sort() %>%  unique()
d1 <- setdiff(tokenQ1, tokenQ2 )  %>% sort() %>%  unique()
d2 <- setdiff(tokenQ2, tokenQ1 )  %>% sort() %>%  unique()
switch( Option,
'1' = {
words_of_d1  <- d2      %>% word_vectors[., , drop = FALSE]
words_of_q2  <- tokenQ1 %>% word_vectors[., , drop = FALSE]
word_interest <- rbind(d2, tokenQ1)  %>% sort() %>%  unique()
} ,
'2' = {
words_of_d1  <- d1      %>% word_vectors[., , drop = FALSE]
words_of_q2  <- tokenQ2 %>% word_vectors[., , drop = FALSE]
word_interest <- rbind(d1, tokenQ2)  %>% sort() %>%  unique()
} ,
default = {
words_of_d1  <- d1      %>% word_vectors[., , drop = FALSE]
words_of_q2  <- tokenQ2 %>% word_vectors[., , drop = FALSE]
word_interest <- rbind(d1, tokenQ2)  %>% sort() %>%  unique()
}
)
word_interest_v <- word_interest %>%  word_vectors[., , drop = FALSE]
set.seed(50)
tsne_model_1 = word_interest_v %>%  as.matrix( ) %>%  Rtsne(. , check_duplicates=FALSE, pca=TRUE, perplexity=3, theta=0.5, dims=2)
## getting the two dimension matrix
d_tsne_1 = data.frame(tsne_model_1$Y, term =row.names(word_interest_v) )
row.names(d_tsne_1) =  row.names(word_interest_v)
ggplot(d_tsne_1, aes(x= X1, y = X2)) +
geom_point(color = "blue", size = 1) +
geom_label_repel(aes(label = term),
segment.color = "grey50") +
geom_segment(aes(x = 0, y = 0, xend = X1, yend = X2), data = d_tsne_1[tokenQ2,-3],arrow = arrow())+
geom_segment(aes(x = 0, y = 0, xend = X1, yend = X2, color="red"), data = d_tsne_1[d1,-3],arrow = arrow())+
geom_vline(xintercept = 0,color = "blue")+
geom_hline(yintercept = 0,color = "blue")+
theme_classic()
}
get_word_vectors() %>% plot_word_vector_questions_vectors(df[10,],.,'3')
plot_word_vector_questions_vectors <- function(OnRaw,word_vectors,Option)
{
tokenQ1 =   tokenize_words( OnRaw["question1"] ) %>% unlist() %>% sort() %>%  unique()
tokenQ2 =   tokenize_words( OnRaw["question2"] ) %>% unlist() %>% sort() %>%  unique()
d1 <- setdiff(tokenQ1, tokenQ2 )  %>% sort() %>%  unique()
d2 <- setdiff(tokenQ2, tokenQ1 )  %>% sort() %>%  unique()
switch( Option,
'1' = {
words_of_d1  <- d2      %>% word_vectors[., , drop = FALSE]
words_of_q2  <- tokenQ1 %>% word_vectors[., , drop = FALSE]
word_interest <- rbind(d2, tokenQ1)  %>% sort() %>%  unique()
} ,
'2' = {
words_of_d1  <- d1      %>% word_vectors[., , drop = FALSE]
words_of_q2  <- tokenQ2 %>% word_vectors[., , drop = FALSE]
word_interest <- rbind(d1, tokenQ2)  %>% sort() %>%  unique()
}
)
word_interest_v <- word_interest %>%  word_vectors[., , drop = FALSE]
set.seed(50)
tsne_model_1 = word_interest_v %>%  as.matrix( ) %>%  Rtsne(. , check_duplicates=FALSE, pca=TRUE, perplexity=3, theta=0.5, dims=2)
## getting the two dimension matrix
d_tsne_1 = data.frame(tsne_model_1$Y, term =row.names(word_interest_v) )
row.names(d_tsne_1) =  row.names(word_interest_v)
ggplot(d_tsne_1, aes(x= X1, y = X2)) +
geom_point(color = "blue", size = 1) +
geom_label_repel(aes(label = term),
segment.color = "grey50") +
geom_segment(aes(x = 0, y = 0, xend = X1, yend = X2), data = d_tsne_1[tokenQ2,-3],arrow = arrow())+
geom_segment(aes(x = 0, y = 0, xend = X1, yend = X2, color="red"), data = d_tsne_1[d1,-3],arrow = arrow())+
geom_vline(xintercept = 0,color = "blue")+
geom_hline(yintercept = 0,color = "blue")+
theme_classic()
}
get_word_vectors() %>% plot_word_vector_questions_vectors(df[10,],.,'3')
get_word_vectors() %>% plot_word_vector_questions_vectors(df[10,],.,'2')
runApp()
runApp()
plot_word_vector_questions_vectors <- function(OnRaw,word_vectors,Option)
{
tokenQ1 =   tokenize_words( OnRaw["question1"] ) %>% unlist() %>% sort() %>%  unique()
tokenQ2 =   tokenize_words( OnRaw["question2"] ) %>% unlist() %>% sort() %>%  unique()
d1 <- setdiff(tokenQ1, tokenQ2 )  %>% sort() %>%  unique()
d2 <- setdiff(tokenQ2, tokenQ1 )  %>% sort() %>%  unique()
switch( Option,
1 = {
words_of_d1  <- d2      %>% word_vectors[., , drop = FALSE]
words_of_q2  <- tokenQ1 %>% word_vectors[., , drop = FALSE]
word_interest <- rbind(d2, tokenQ1)  %>% sort() %>%  unique()
} ,
2 = {
words_of_d1  <- d1      %>% word_vectors[., , drop = FALSE]
words_of_q2  <- tokenQ2 %>% word_vectors[., , drop = FALSE]
word_interest <- rbind(d1, tokenQ2)  %>% sort() %>%  unique()
}
)
word_interest_v <- word_interest %>%  word_vectors[., , drop = FALSE]
set.seed(50)
tsne_model_1 = word_interest_v %>%  as.matrix( ) %>%  Rtsne(. , check_duplicates=FALSE, pca=TRUE, perplexity=3, theta=0.5, dims=2)
## getting the two dimension matrix
d_tsne_1 = data.frame(tsne_model_1$Y, term =row.names(word_interest_v) )
row.names(d_tsne_1) =  row.names(word_interest_v)
ggplot(d_tsne_1, aes(x= X1, y = X2)) +
geom_point(color = "blue", size = 1) +
geom_label_repel(aes(label = term),
segment.color = "grey50") +
geom_segment(aes(x = 0, y = 0, xend = X1, yend = X2), data = d_tsne_1[tokenQ2,-3],arrow = arrow())+
geom_segment(aes(x = 0, y = 0, xend = X1, yend = X2, color="red"), data = d_tsne_1[d1,-3],arrow = arrow())+
geom_vline(xintercept = 0,color = "blue")+
geom_hline(yintercept = 0,color = "blue")+
theme_classic()
}
runApp()
glove_create_vocab<- function(itokens)
{
vocab <- itokens %>% create_vocabulary( )
# vocab <- prune_vocabulary(vocab, term_count_min = 1L)
list(vocab=vocab, itokens = itokens)
}
glove_create_vectorizer <- function(parameters)
{
# Use our filtered vocabulary
vectorizer <- parameters$vocab %>% vocab_vectorizer( )
list( vectorizer = vectorizer,
vocab= parameters$vocab,
itokens = parameters$itokens)
}
glove_create_tcm <- function(parameters)
{
# use window of 5 for context words
tcm <- parameters$itokens %>% create_tcm(parameters$vectorizer,
skip_grams_window = 5L)
list(tcm = tcm,
vocab= parameters$vocab)
}
glove_get_word_vector <- function(parameters)
{
glove = GlobalVectors$new( word_vectors_size = 100,
vocabulary = parameters$vocab, x_max = 10)
wv_main <- glove$fit_transform(parameters$tcm, n_iter = 20)
wv_context = glove$components
word_vectors <- wv_main + t(wv_context)
word_vectors
}
merge_with_pretrained_word_vector <- function( word_vectors_glove,embeddings_index) {
vect_words <- word_vectors_glove %>% row.names() %>% as.character() %>% as.data.frame( )
colnames(vect_words) = c('word')
row.names(vect_words )= vect_words$word
vect_words <- vect_words %>% mutate(word = as.character(word))
i = 1
for( word in vect_words$word)
{
wvec = embeddings_index[[word]]
if( ! wvec %>% is.null() ) word_vectors_glove[i,] = wvec
i = i + 1
}
#   Add two empty vectors at the end
v <- rep_len(0.0,length.out = 100)
word_vectors_glove <-  rbind(word_vectors_glove,v)
word_vectors_glove <-  rbind(word_vectors_glove,v)
word_vectors_glove <-  rbind(word_vectors_glove,v)
word_vectors_glove
}
word_vectors_glove <- rbind(df$question1,df$question2) %>%
tokenize_text_() %>%
glove_create_vocab() %>%
glove_create_vectorizer() %>%
glove_create_tcm() %>%
glove_get_word_vector()
tokenize_text_ <- function(texts_)
{
# Create iterator over tokens
it <- texts_ %>% space_tokenizer( ) %>% itoken( progressbar = FALSE)
it
}
word_vectors_glove <- rbind(df$question1,df$question2) %>%
tokenize_text_() %>%
glove_create_vocab() %>%
glove_create_vectorizer() %>%
glove_create_tcm() %>%
glove_get_word_vector()
df[1:10,"question1"]
tokenize_text_ <- function(parameters)
{
# Create iterator over tokens
it <- parameters$texts_ %>% space_tokenizer( ) %>% itoken( progressbar = FALSE)
it
}
glove_create_vocab<- function(parameters)
{
vocab <- parameters$itokens %>% create_vocabulary( )
# vocab <- prune_vocabulary(vocab, term_count_min = 1L)
list(vocab=vocab, itokens = parameters$itokens)
}
glove_create_vectorizer <- function(parameters)
{
# Use our filtered vocabulary
vectorizer <- parameters$vocab %>% vocab_vectorizer( )
list( vectorizer = vectorizer,
vocab= parameters$vocab,
itokens = parameters$itokens)
}
glove_create_tcm <- function(parameters)
{
# use window of 5 for context words
tcm <- parameters$itokens %>% create_tcm(parameters$vectorizer,
skip_grams_window = 5L)
list(tcm = tcm,
vocab= parameters$vocab)
}
glove_get_word_vector <- function(parameters)
{
glove = GlobalVectors$new( word_vectors_size = 100,
vocabulary = parameters$vocab, x_max = 10)
wv_main <- glove$fit_transform(parameters$tcm, n_iter = 20)
wv_context = glove$components
word_vectors <- wv_main + t(wv_context)
word_vectors
}
merge_with_pretrained_word_vector <- function( word_vectors_glove,embeddings_index) {
vect_words <- word_vectors_glove %>% row.names() %>% as.character() %>% as.data.frame( )
colnames(vect_words) = c('word')
row.names(vect_words )= vect_words$word
vect_words <- vect_words %>% mutate(word = as.character(word))
i = 1
for( word in vect_words$word)
{
wvec = embeddings_index[[word]]
if( ! wvec %>% is.null() ) word_vectors_glove[i,] = wvec
i = i + 1
}
#   Add two empty vectors at the end
v <- rep_len(0.0,length.out = 100)
word_vectors_glove <-  rbind(word_vectors_glove,v)
word_vectors_glove <-  rbind(word_vectors_glove,v)
word_vectors_glove <-  rbind(word_vectors_glove,v)
word_vectors_glove
}
rbind(df[,"question1"],df[,"question2"]) %>%
tokenize_text_() %>%
glove_create_vocab() %>%
glove_create_vectorizer() %>%
glove_create_tcm() %>%
glove_get_word_vector()
tokenize_text_ <- function(parameters)
{
# Create iterator over tokens
it <- parameters$texts_ %>% space_tokenizer( ) %>% itoken( progressbar = FALSE)
list(itokens = it,
skip_grams_window = parameters$skip_grams_window,
x_max = parameters$x_max ,
n_iter = parameters$n_iter ,
word_vectors_size = parameters$word_vectors_size)
}
glove_create_vocab<- function(parameters)
{
vocab <- parameters$itokens %>% create_vocabulary( )
# vocab <- prune_vocabulary(vocab, term_count_min = 1L)
list(vocab=vocab,
itokens = parameters$itokens,
skip_grams_window = parameters$skip_grams_window,
x_max = parameters$x_max ,
n_iter = parameters$n_iter ,
word_vectors_size = parameters$word_vectors_size)
}
glove_create_vectorizer <- function(parameters)
{
# Use our filtered vocabulary
vectorizer <- parameters$vocab %>% vocab_vectorizer( )
list( vectorizer = vectorizer,
vocab= parameters$vocab,
itokens = parameters$itokens,
skip_grams_window = parameters$skip_grams_window,
x_max = parameters$x_max ,
n_iter = parameters$n_iter ,
word_vectors_size = parameters$word_vectors_size)
}
glove_create_tcm <- function(parameters)
{
# use window of 5 for context words
tcm <- parameters$itokens %>% create_tcm(parameters$vectorizer,
skip_grams_window = parameters$skip_grams_window)
list(tcm = tcm,
vocab= parameters$vocab,
skip_grams_window = parameters$skip_grams_window,
x_max = parameters$x_max ,
n_iter = parameters$n_iter ,
word_vectors_size = parameters$word_vectors_size)
}
glove_get_word_vector <- function(parameters)
{
glove = GlobalVectors$new( word_vectors_size = parameters$word_vectors_size,
vocabulary = parameters$vocab, x_max = parameters$x_max)
wv_main <- glove$fit_transform(parameters$tcm, n_iter = parameters$n_iter)
wv_context = glove$components
word_vectors <- wv_main + t(wv_context)
word_vectors
}
merge_with_pretrained_word_vector <- function( word_vectors_glove,embeddings_index) {
vect_words <- word_vectors_glove %>% row.names() %>% as.character() %>% as.data.frame( )
colnames(vect_words) = c('word')
row.names(vect_words )= vect_words$word
vect_words <- vect_words %>% mutate(word = as.character(word))
i = 1
for( word in vect_words$word)
{
wvec = embeddings_index[[word]]
if( ! wvec %>% is.null() ) word_vectors_glove[i,] = wvec
i = i + 1
}
#   Add two empty vectors at the end
v <- rep_len(0.0,length.out = 100)
word_vectors_glove <-  rbind(word_vectors_glove,v)
word_vectors_glove <-  rbind(word_vectors_glove,v)
word_vectors_glove <-  rbind(word_vectors_glove,v)
word_vectors_glove
}
list(texts_ = rbind(df[,"question1"],df[,"question2"]),skip_grams_window = 5 , x_max = 10 , n_iter = 2 , word_vectors_size = 100) %>%
tokenize_text_() %>%
glove_create_vocab() %>%
glove_create_vectorizer() %>%
glove_create_tcm() %>%
glove_get_word_vector()
list(a = 1, b = 2 ) %>% cbind(list(c=3,d=5))
list(a = 1, b = 2 ) %>% rbind(list(c=3,d=5))
list(a = 1, b = 2 ) %>% c(.,list(c=3,d=5))
tokenize_text_ <- function(parameters)
{
# Create iterator over tokens
it <- parameters$texts_ %>% space_tokenizer( ) %>% itoken( progressbar = FALSE)
list(itokens = it ) %>% c(.,parameters)
# skip_grams_window = parameters$skip_grams_window,
# x_max = parameters$x_max ,
# n_iter = parameters$n_iter ,
# word_vectors_size = parameters$word_vectors_size)
}
list(texts_ = rbind(df[,"question1"],df[,"question2"]),skip_grams_window = 5 , x_max = 10 , n_iter = 2 , word_vectors_size = 100) %>%
tokenize_text_() %>%
glove_create_vocab() %>%
glove_create_vectorizer() %>%
glove_create_tcm() %>%
glove_get_word_vector()
tokenize_text_ <- function(parameters)
{
# Create iterator over tokens
it <- parameters$texts_ %>% space_tokenizer( ) %>% itoken( progressbar = FALSE)
list(itokens = it ) %>% c(.,parameters)
# skip_grams_window = parameters$skip_grams_window,
# x_max = parameters$x_max ,
# n_iter = parameters$n_iter ,
# word_vectors_size = parameters$word_vectors_size)
}
glove_create_vocab<- function(parameters)
{
vocab <- parameters$itokens %>% create_vocabulary( )
# vocab <- prune_vocabulary(vocab, term_count_min = 1L)
list(vocab=vocab ) %>%  c(.,parameters)
}
glove_create_vectorizer <- function(parameters)
{
# Use our filtered vocabulary
vectorizer <- parameters$vocab %>% vocab_vectorizer( )
list( vectorizer = vectorizer) %>% c(. , parameters )
}
glove_create_tcm <- function(parameters)
{
# use window of 5 for context words
tcm <- parameters$itokens %>% create_tcm(parameters$vectorizer,
skip_grams_window = parameters$skip_grams_window)
list(tcm = tcm ) %>% c(.,parameters)
}
glove_get_word_vector <- function(parameters)
{
glove = GlobalVectors$new( word_vectors_size = parameters$word_vectors_size,
vocabulary = parameters$vocab, x_max = parameters$x_max)
wv_main <- glove$fit_transform(parameters$tcm, n_iter = parameters$n_iter)
wv_context = glove$components
word_vectors <- wv_main + t(wv_context)
word_vectors
}
merge_with_pretrained_word_vector <- function( word_vectors_glove,embeddings_index) {
vect_words <- word_vectors_glove %>% row.names() %>% as.character() %>% as.data.frame( )
colnames(vect_words) = c('word')
row.names(vect_words )= vect_words$word
vect_words <- vect_words %>% mutate(word = as.character(word))
i = 1
for( word in vect_words$word)
{
wvec = embeddings_index[[word]]
if( ! wvec %>% is.null() ) word_vectors_glove[i,] = wvec
i = i + 1
}
#   Add two empty vectors at the end
v <- rep_len(0.0,length.out = 100)
word_vectors_glove <-  rbind(word_vectors_glove,v)
word_vectors_glove <-  rbind(word_vectors_glove,v)
word_vectors_glove <-  rbind(word_vectors_glove,v)
word_vectors_glove
}
list(texts_ = rbind(df[,"question1"],df[,"question2"]),skip_grams_window = 5 , x_max = 10 , n_iter = 2 , word_vectors_size = 100) %>%
tokenize_text_() %>%
glove_create_vocab() %>%
glove_create_vectorizer() %>%
glove_create_tcm() %>%
glove_get_word_vector()
runApp()
list(texts_ = rbind(df[,"question1"],df[,"question2"]),skip_grams_window = 5 , x_max = 10 , n_iter = 2 , word_vectors_size = 100) %>%
tokenize_text_() %>%
glove_create_vocab() %>%
glove_create_vectorizer() %>%
glove_create_tcm() %>%
glove_get_word_vector()
list(texts_ = rbind(df[1:1000,"question1"],df[1:1000,"question2"]),skip_grams_window = 5 , x_max = 10 , n_iter = 2 , word_vectors_size = 100) %>%
tokenize_text_() %>%
glove_create_vocab() %>%
glove_create_vectorizer() %>%
glove_create_tcm() %>%
glove_get_word_vector()
list(texts_ = rbind(df[1:10000,"question1"],df[1:10000,"question2"]),skip_grams_window = 5 , x_max = 10 , n_iter = 2 , word_vectors_size = 100) %>%
tokenize_text_() %>%
glove_create_vocab() %>%
glove_create_vectorizer() %>%
glove_create_tcm() %>%
glove_get_word_vector()
list(texts_ = rbind(df[1:100000,"question1"],df[1:100000,"question2"]),skip_grams_window = 5 , x_max = 10 , n_iter = 2 , word_vectors_size = 100) %>%
tokenize_text_() %>%
glove_create_vocab() %>%
glove_create_vectorizer() %>%
glove_create_tcm() %>%
glove_get_word_vector()
runApp()
